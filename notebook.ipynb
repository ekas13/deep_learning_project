{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = MNIST(\"./temp/\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = MNIST(\"./temp/\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up training we'll only work on a subset of the data\n",
    "x_train_set = mnist_trainset.data.view(-1, 784).float()\n",
    "x_test = mnist_testset.data.view(-1, 784).float()\n",
    "x_train = torch.cat((x_train_set, x_test), dim=0)\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "\n",
    "# normalize the inputs\n",
    "x_train.div_(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNetwork0(torch.nn.Module):\n",
    "    # takes an input image and time, returns the score function\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nch = 2\n",
    "        chs = [32, 64, 128, 256, 256]\n",
    "        self._convs = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(2, chs[0], kernel_size=3, padding=1),  # (batch, ch, 28, 28)\n",
    "                torch.nn.LogSigmoid(),  # (batch, 8, 28, 28)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=2, stride=2),  # (batch, ch, 14, 14)\n",
    "                torch.nn.Conv2d(chs[0], chs[1], kernel_size=3, padding=1),  # (batch, ch, 14, 14)\n",
    "                torch.nn.LogSigmoid(),  # (batch, 16, 14, 14)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=2, stride=2),  # (batch, ch, 7, 7)\n",
    "                torch.nn.Conv2d(chs[1], chs[2], kernel_size=3, padding=1),  # (batch, ch, 7, 7)\n",
    "                torch.nn.LogSigmoid(),  # (batch, 32, 7, 7)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),  # (batch, ch, 4, 4)\n",
    "                torch.nn.Conv2d(chs[2], chs[3], kernel_size=3, padding=1),  # (batch, ch, 4, 4)\n",
    "                torch.nn.LogSigmoid(),  # (batch, 64, 4, 4)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=2, stride=2),  # (batch, ch, 2, 2)\n",
    "                torch.nn.Conv2d(chs[3], chs[4], kernel_size=3, padding=1),  # (batch, ch, 2, 2)\n",
    "                torch.nn.LogSigmoid(),  # (batch, 64, 2, 2)\n",
    "            ),\n",
    "        ])\n",
    "        self._tconvs = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                # input is the output of convs[4]\n",
    "                torch.nn.ConvTranspose2d(chs[4], chs[3], kernel_size=3, stride=2, padding=1, output_padding=1),  # (batch, 64, 4, 4)\n",
    "                torch.nn.LogSigmoid(),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                # input is the output from the above sequential concated with the output from convs[3]\n",
    "                torch.nn.ConvTranspose2d(chs[3] * 2, chs[2], kernel_size=3, stride=2, padding=1, output_padding=0),  # (batch, 32, 7, 7)\n",
    "                torch.nn.LogSigmoid(),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                # input is the output from the above sequential concated with the output from convs[2]\n",
    "                torch.nn.ConvTranspose2d(chs[2] * 2, chs[1], kernel_size=3, stride=2, padding=1, output_padding=1),  # (batch, chs[2], 14, 14)\n",
    "                torch.nn.LogSigmoid(),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                # input is the output from the above sequential concated with the output from convs[1]\n",
    "                torch.nn.ConvTranspose2d(chs[1] * 2, chs[0], kernel_size=3, stride=2, padding=1, output_padding=1),  # (batch, chs[1], 28, 28)\n",
    "                torch.nn.LogSigmoid(),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                # input is the output from the above sequential concated with the output from convs[0]\n",
    "                torch.nn.Conv2d(chs[0] * 2, chs[0], kernel_size=3, padding=1),  # (batch, chs[0], 28, 28)\n",
    "                torch.nn.LogSigmoid(),\n",
    "                torch.nn.Conv2d(chs[0], 1, kernel_size=3, padding=1),  # (batch, 1, 28, 28)\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., ch0 * 28 * 28), t: (..., 1)\n",
    "        x2 = torch.reshape(x, (*x.shape[:-1], 1, 28, 28))  # (..., ch0, 28, 28)\n",
    "        tt = t[..., None, None].expand(*t.shape[:-1], 1, 28, 28)  # (..., 1, 28, 28)\n",
    "        x2t = torch.cat((x2, tt), dim=-3)\n",
    "        signal = x2t\n",
    "        signals = []\n",
    "        for i, conv in enumerate(self._convs):\n",
    "            signal = conv(signal)\n",
    "            if i < len(self._convs) - 1:\n",
    "                signals.append(signal)\n",
    "\n",
    "        for i, tconv in enumerate(self._tconvs):\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "        signal = torch.reshape(signal, (*signal.shape[:-3], -1))  # (..., 1 * 28 * 28)\n",
    "        return signal\n",
    "\n",
    "score_network = ScoreNetwork0().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define loss and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "opt = torch.optim.AdamW(score_network.parameters(), lr=2e-4)\n",
    "\n",
    "#loss\n",
    "loss_MSE = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of transformations in Forward process\n",
    "T = 1000\n",
    "#variance schedule \n",
    "betas = torch.linspace(0.0001, 0.02, T)  #myb should be sorted or we can check out the SDE thing so we need no variance schedule\n",
    "#the cumulative alpha \n",
    "alphas = 1 - betas\n",
    "alpha_calculate_cumulative = lambda idx: torch.prod(alphas[:idx+1])\n",
    "#epochs\n",
    "max_epochs = 100\n",
    "#convergence check\n",
    "convergence_threshold = 1e-6 \n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "# number of batches\n",
    "num_batches = x_train.shape[0] // batch_size\n",
    "print(\"num_batches\", num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traininig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(losses):\n",
    "    \n",
    "    get_slice = lambda i,  size: range(i * size, (i + 1) * size)\n",
    "\n",
    "    previous_loss = 0\n",
    "    for i in range(max_epochs):\n",
    "        current_loss = []\n",
    "        for j in range(num_batches):\n",
    "            slce = get_slice(j, batch_size) \n",
    "            #sample training params \n",
    "            x_0_np = x_train[slce] \n",
    "            x_0 = torch.tensor(x_0_np).to(device)  # Convert to tensor and move to device\n",
    "            x_0 = x_0.view(batch_size, 28*28)\n",
    "\n",
    "            #t = torch.rand((1, 1), dtype=x_0.dtype, device=x_0.device) * (1 - 1e-4) + 1e-4\n",
    "            #t = torch.randint(1, T + 1, (1, 1), dtype=x_0.dtype, device=x_0.device) / T\n",
    "            t = torch.randint(1, T + 1, (batch_size, 1), dtype=torch.int64, device=x_0.device)\n",
    "\n",
    "            epsilon = torch.randn_like(x_0, device=device)   #N(0,1)\n",
    "            \n",
    "            #create noisy observation\n",
    "            alpha_ = [alpha_calculate_cumulative(time).detach().to(device) for time in t]\n",
    "            alpha_ = torch.tensor(alpha_, device=device).view(-1, 1)  \n",
    "            x_noisy = torch.sqrt(alpha_)*x_0 + torch.sqrt(1-alpha_)*epsilon\n",
    "            \n",
    "            #train\n",
    "            epsilon_hat = score_network(x_noisy, t)  #forward pass\n",
    "            batch_loss = loss_MSE(epsilon, epsilon_hat)    #calculate loss\n",
    "            opt.zero_grad()                          #reset grad\n",
    "            batch_loss.backward()                          #backprop\n",
    "            opt.step()                               #train params\n",
    "            current_loss.append(batch_loss.item())\n",
    "        losses.append(np.mean(current_loss))\n",
    "\n",
    "        #stop training until loss converged\n",
    "        \n",
    "        print(f\"Epoch {i+1} | Loss {losses[-1]}\")\n",
    "        # if abs(previous_loss - current_loss) < convergence_threshold:\n",
    "        #     break\n",
    "        # previous_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over time.\n",
    "\n",
    "    Parameters:\n",
    "    losses (list or array): A list of loss values over epochs or iterations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Training Loss', color='blue', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "losses = []\n",
    "training(losses)\n",
    "plot_loss(losses)\n",
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling():\n",
    "    with torch.no_grad(): #turn of grad \n",
    "        score_network.eval() #turn of dropout and similar\n",
    "\n",
    "        #generate noise sample\n",
    "        x_T = x_previous_t = torch.randn(1, 784, device=device)\n",
    "        x_0 = None\n",
    "        #removing the noise for each transition\n",
    "        for t in range(T-1, 0, -1):\n",
    "\n",
    "            #set noise \n",
    "            z =  torch.zeros_like(x_T)      #special case when it's the last transition 1->0\n",
    "            if t > 1:\n",
    "                z = torch.randn_like(x_T)   #otherwise, N(0,1) \n",
    "\n",
    "            #remove noise for this timestep transition\n",
    "            alpha_t = alphas[t]\n",
    "            beta_t = betas[t]\n",
    "            alpha_cum_t = alpha_calculate_cumulative(t).item()\n",
    "            variance_t = beta_t             #variance of p_theta we have to choose based on x_0 - for now this since x_0 ~ N(0,I) \n",
    "            \n",
    "            alpha_cum_t_minus_1 = alpha_calculate_cumulative(t - 1).item()  # Cumulative product up to t-1\n",
    "\n",
    "            # Variance term for DDPM, incorporating cumulative alphas for stability\n",
    "            #variance_t = beta_t * (1 - alpha_cum_t_minus_1) / (1 - alpha_cum_t)\n",
    "\n",
    "            time = torch.tensor([[t]], dtype=torch.int64, device=x_T.device)\n",
    "            epsilon_hat = score_network(x_previous_t, time)\n",
    "\n",
    "            # x_previous_t = (1/np.sqrt(alpha_t))*(x_previous_t - ((1-alpha_t)/np.sqrt(1-alpha_cum_t))*epsilon_hat) + (variance_t*z)\n",
    "            x_previous_t = (1 / np.sqrt(alpha_t)) * (x_previous_t - ((1 - alpha_t) / np.sqrt(1 - alpha_cum_t)) * epsilon_hat) + (np.sqrt(variance_t) * z)\n",
    "            \n",
    "            x_0 = x_previous_t #remember last for return\n",
    "\n",
    "    #return final calculated x_0\n",
    "    return x_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few MNIST examples\n",
    "idx, dim, classes = 0, 28, 5\n",
    "\n",
    "# create empty canvas\n",
    "canvas = np.zeros((dim * classes, classes * dim))\n",
    "\n",
    "# fill with tensors\n",
    "for i in range(classes):\n",
    "    for j in range(classes):\n",
    "        # Detach the tensor and convert it to a NumPy array\n",
    "        canvas[i * dim:(i + 1) * dim, j * dim:(j + 1) * dim] = sampling().cpu().detach().reshape((dim, dim)).numpy()\n",
    "\n",
    "    print(str(i) + ' sample')\n",
    "\n",
    "# visualize matrix of tensors as gray scale image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def fid_score(sampled_data, real_data):\n",
    "    model = inception_v3(pretrained=True, transform_input=False).to(device).eval()\n",
    "\n",
    "    sampled_images = torch.nn.functional.interpolate(sampled_data, size=(28, 28), mode='bilinear')\n",
    "    sampled_images = F.interpolate(sampled_images, size=(299, 299), mode='bilinear')\n",
    "    sampled_images = sampled_images.repeat(1, 3, 1, 1) # turn into rgb for model\n",
    "    \n",
    "    real_images = torch.nn.functional.interpolate(real_data, size=(28, 28), mode='bilinear')\n",
    "    real_images = F.interpolate(real_images, size=(299, 299), mode='bilinear')\n",
    "    real_images = real_images.repeat(1, 3, 1, 1) # turn into rgb for model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sampled_features = model(sampled_images).cpu().numpy()\n",
    "        real_features = model(real_images).cpu().numpy()\n",
    "\n",
    "    mu_sampled = np.mean(sampled_features, axis=0)\n",
    "    cov_sampled = np.cov(sampled_features, rowvar=False)\n",
    "\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    cov_real = np.cov(real_features, rowvar=False)\n",
    "\n",
    "    mu_diff = mu_sampled - mu_real\n",
    "    cov_sqrtm = sqrtm(cov_sampled.dot(cov_real))\n",
    "    return mu_diff.dot(mu_diff) + np.trace(cov_sampled + cov_real - 2 * cov_sqrtm)\n",
    "\n",
    "def inception_score(sampled_data, real_data):\n",
    "    model = inception_v3(pretrained=True, transform_input=False).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genearte samples\n",
    "num_samples = 10000\n",
    "\n",
    "sampled_data = torch.cat([sampling() for _ in range(num_samples)], dim=0).view(-1, 1, 28, 28).to(device)\n",
    "real_data = x_train[:num_samples].view(-1, 1, 28, 28).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fid = fid_score(sampled_data, real_data)\n",
    "print(f'FID score: {fid}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
